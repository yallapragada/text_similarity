{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_similarity_doc2vec_annoy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yallapragada/text_similarity/blob/master/text_similarity_doc2vec_annoy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU7uq3yt_hSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCnvaS5ewLId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "549fa4f9-9182-4b13-c583-e4e0ded71445"
      },
      "source": [
        "!pip install stop_words\n",
        "from stop_words import get_stop_words"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stop_words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-cp36-none-any.whl size=32916 sha256=28acd46d4d000fbffc2c98e5450dd9e4cc778fd1b9127cd8ea21f80d70e91999\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkc7lC9KAXpH",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "5f909cac-bb9a-4d84-c026-63cbef88af82"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-60e4b779-6788-4f5c-ae41-c853b09d146c\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-60e4b779-6788-4f5c-ae41-c853b09d146c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving arxivData.json to arxivData.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdOM5YVMvaNB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "70257368-fb86-40e9-a8f8-1dcb37772af9"
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "df = pd.read_json(io.StringIO(uploaded['arxivData.json'].decode('ISO-8859-1')))\n",
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>day</th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>month</th>\n",
              "      <th>summary</th>\n",
              "      <th>tag</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[{'name': 'Ahmed Osman'}, {'name': 'Wojciech S...</td>\n",
              "      <td>1</td>\n",
              "      <td>1802.00209v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>2</td>\n",
              "      <td>We propose an architecture for VQA which utili...</td>\n",
              "      <td>[{'term': 'cs.AI', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Dual Recurrent Attention Units for Visual Ques...</td>\n",
              "      <td>2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[{'name': 'Ji Young Lee'}, {'name': 'Franck De...</td>\n",
              "      <td>12</td>\n",
              "      <td>1603.03827v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>3</td>\n",
              "      <td>Recent approaches based on artificial neural n...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Sequential Short-Text Classification with Recu...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[{'name': 'Iulian Vlad Serban'}, {'name': 'Tim...</td>\n",
              "      <td>2</td>\n",
              "      <td>1606.00776v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>6</td>\n",
              "      <td>We introduce the multiresolution recurrent neu...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Multiresolution Recurrent Neural Networks: An ...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[{'name': 'Sebastian Ruder'}, {'name': 'Joachi...</td>\n",
              "      <td>23</td>\n",
              "      <td>1705.08142v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>5</td>\n",
              "      <td>Multi-task learning is motivated by the observ...</td>\n",
              "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>Learning what to share between loosely related...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[{'name': 'Iulian V. Serban'}, {'name': 'Chinn...</td>\n",
              "      <td>7</td>\n",
              "      <td>1709.02349v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>9</td>\n",
              "      <td>We present MILABOT: a deep reinforcement learn...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>A Deep Reinforcement Learning Chatbot</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              author  ...  year\n",
              "0  [{'name': 'Ahmed Osman'}, {'name': 'Wojciech S...  ...  2018\n",
              "1  [{'name': 'Ji Young Lee'}, {'name': 'Franck De...  ...  2016\n",
              "2  [{'name': 'Iulian Vlad Serban'}, {'name': 'Tim...  ...  2016\n",
              "3  [{'name': 'Sebastian Ruder'}, {'name': 'Joachi...  ...  2017\n",
              "4  [{'name': 'Iulian V. Serban'}, {'name': 'Chinn...  ...  2017\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFzy-lhD2lCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrZPK29V272Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = get_stop_words('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsrMP_n524By",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tagged_document(text, id):\n",
        "  doc = nlp(text)\n",
        "  words = [token.text for token in doc if token.text not in stop_words and len(token.text) > 2]\n",
        "  tagged_document = TaggedDocument(words, tags=[id])\n",
        "  return tagged_document"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltq_YfWTyg4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tagged_documents = []\n",
        "for i,row in df.iterrows():\n",
        "  if i % 500 == 0:\n",
        "    print(i)\n",
        "  text = row['title'] + ' ' + row['summary']\n",
        "  text = text.lower()\n",
        "  id = row['id']\n",
        "  tagged_document = get_tagged_document(text=text, id=id)\n",
        "  tagged_documents.append(tagged_document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsoQ2vdghxG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tagged_documents = [tagged_document for tagged_document in tagged_documents if (len(tagged_document.words) > 15)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zz2HYedip9x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3bdd59a-5bc8-4242-b5db-77e2e7c63199"
      },
      "source": [
        "len(tagged_documents)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40953"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frf4tGT8_elh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Doc2Vec(tagged_documents, vector_size=100, window=5, min_count=1, workers=4, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-obDCaEYAvA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "fdc286ac-996d-4ca6-9795-726790a6aad9"
      },
      "source": [
        "!pip install annoy\n",
        "from gensim.similarities.index import AnnoyIndexer"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting annoy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/4d/5bfeb440265e5b1ae77388d67ea453403bbab1a6fb9b53cdfd3d76b5c320/annoy-1.16.0.tar.gz (636kB)\n",
            "\r\u001b[K     |▌                               | 10kB 14.9MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 4.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 4.8MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 122kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 133kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 153kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 163kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 174kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 184kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 194kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 204kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 215kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 225kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 235kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 245kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 256kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 266kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 276kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 286kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 296kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 307kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 317kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 327kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 337kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 348kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 358kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 368kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 378kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 389kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 399kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 409kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 419kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 430kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 440kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 450kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 460kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 471kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 481kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 491kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 501kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 512kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 522kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 532kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 542kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 552kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 563kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 573kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 583kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 593kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 604kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 614kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 624kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 634kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 4.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.16.0-cp36-cp36m-linux_x86_64.whl size=309746 sha256=4f4a25659f2b158e127088a1578bbcf29c77308a41b2f1f43fca4b2778d1db28\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/34/9c/8d8eb3abce9375bade7b42bca387700b5f3aa18414d27272f0\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.16.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh2yneFSA4ZX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c231e268-0bf0-4378-ca5d-ba7def2b263a"
      },
      "source": [
        "annoy_index = AnnoyIndexer(model, 30)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/similarities/index.py:180: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
            "  index = AnnoyIndex(num_features)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-Kv2j2fdHCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "873e883b-4a6e-4c92-8791-b140b0533b56"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx3nrPE_el1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_doc = '''\n",
        "In this paper, we consider the task of\n",
        "text categorization as a graph classification problem. By representing textual documents as graph-of-words instead of historical n-gram bag-of-words, we extract\n",
        "more discriminative features that correspond to long-distance n-grams through\n",
        "frequent subgraph mining. Moreover, by\n",
        "capitalizing on the concept of k-core, we\n",
        "reduce the graph representation to its densest part – its main core – speeding up the\n",
        "feature extraction step for little to no cost\n",
        "in prediction performances. Experiments\n",
        "on four standard text classification datasets\n",
        "show statistically significant higher accuracy and macro-averaged F1-score compared to baseline approaches.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Kr5BK6evcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_doc = test_doc.replace('\\n', ' ').lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHREB9K5e4j-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f781877d-ff3b-4aa4-ba1b-3652c6a8978b"
      },
      "source": [
        "test_doc"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' in this paper, we consider the task of text categorization as a graph classification problem. by representing textual documents as graph-of-words instead of historical n-gram bag-of-words, we extract more discriminative features that correspond to long-distance n-grams through frequent subgraph mining. moreover, by capitalizing on the concept of k-core, we reduce the graph representation to its densest part – its main core – speeding up the feature extraction step for little to no cost in prediction performances. experiments on four standard text classification datasets show statistically significant higher accuracy and macro-averaged f1-score compared to baseline approaches. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfG6GJNzfENk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(test_doc)\n",
        "words = [token.text for token in doc if token.text not in stop_words and len(token.text) > 2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjc6BnSpfI66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vector = model.infer_vector(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGcXH0pLfquy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d4b883d-2726-4bdd-9bfd-0cabae2aa0c2"
      },
      "source": [
        "neighbors = model.most_similar([vector], topn=5, indexer=annoy_index)\n",
        "for neighbor in neighbors:\n",
        "    print(df.loc[df['id'] == neighbor[0], 'summary'].iloc[0])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graph classification is a problem with practical applications in many\n",
            "different domains. Most of the existing methods take the entire graph into\n",
            "account when calculating graph features. In a graphlet-based approach, for\n",
            "instance, the entire graph is processed to get the total count of different\n",
            "graphlets or sub-graphs. In the real-world, however, graphs can be both large\n",
            "and noisy with discriminative patterns confined to certain regions in the graph\n",
            "only. In this work, we study the problem of attentional processing for graph\n",
            "classification. The use of attention allows us to focus on small but\n",
            "informative parts of the graph, avoiding noise in the rest of the graph. We\n",
            "present a novel RNN model, called the Graph Attention Model (GAM), that\n",
            "processes only a portion of the graph by adaptively selecting a sequence of\n",
            "\"interesting\" nodes. The model is equipped with an external memory component\n",
            "which allows it to integrate information gathered from different parts of the\n",
            "graph. We demonstrate the effectiveness of the model through various\n",
            "experiments.\n",
            "We present graph partition neural networks (GPNN), an extension of graph\n",
            "neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate\n",
            "between locally propagating information between nodes in small subgraphs and\n",
            "globally propagating information between the subgraphs. To efficiently\n",
            "partition graphs, we experiment with several partitioning algorithms and also\n",
            "propose a novel variant for fast processing of large scale graphs. We\n",
            "extensively test our model on a variety of semi-supervised node classification\n",
            "tasks. Experimental results indicate that GPNNs are either superior or\n",
            "comparable to state-of-the-art methods on a wide variety of datasets for\n",
            "graph-based semi-supervised classification. We also show that GPNNs can achieve\n",
            "similar performance as standard GNNs with fewer propagation steps.\n",
            "In this paper we consider the problem of graph-based transductive\n",
            "classification, and we are particularly interested in the directed graph\n",
            "scenario which is a natural form for many real world applications. Different\n",
            "from existing research efforts that either only deal with undirected graphs or\n",
            "circumvent directionality by means of symmetrization, we propose a novel random\n",
            "walk approach on directed graphs using absorbing Markov chains, which can be\n",
            "regarded as maximizing the accumulated expected number of visits from the\n",
            "unlabeled transient states. Our algorithm is simple, easy to implement, and\n",
            "works with large-scale graphs. In particular, it is capable of preserving the\n",
            "graph structure even when the input graph is sparse and changes over time, as\n",
            "well as retaining weak signals presented in the directed edges. We present its\n",
            "intimate connections to a number of existing methods, including graph kernels,\n",
            "graph Laplacian based methods, and interestingly, spanning forest of graphs.\n",
            "Its computational complexity and the generalization error are also studied.\n",
            "Empirically our algorithm is systematically evaluated on a wide range of\n",
            "applications, where it has shown to perform competitively comparing to a suite\n",
            "of state-of-the-art methods.\n",
            "Recent works on representation learning for graph structured data\n",
            "predominantly focus on learning distributed representations of graph\n",
            "substructures such as nodes and subgraphs. However, many graph analytics tasks\n",
            "such as graph classification and clustering require representing entire graphs\n",
            "as fixed length feature vectors. While the aforementioned approaches are\n",
            "naturally unequipped to learn such representations, graph kernels remain as the\n",
            "most effective way of obtaining them. However, these graph kernels use\n",
            "handcrafted features (e.g., shortest paths, graphlets, etc.) and hence are\n",
            "hampered by problems such as poor generalization. To address this limitation,\n",
            "in this work, we propose a neural embedding framework named graph2vec to learn\n",
            "data-driven distributed representations of arbitrary sized graphs. graph2vec's\n",
            "embeddings are learnt in an unsupervised manner and are task agnostic. Hence,\n",
            "they could be used for any downstream task such as graph classification,\n",
            "clustering and even seeding supervised representation learning approaches. Our\n",
            "experiments on several benchmark and large real-world datasets show that\n",
            "graph2vec achieves significant improvements in classification and clustering\n",
            "accuracies over substructure representation learning approaches and are\n",
            "competitive with state-of-the-art graph kernels.\n",
            "Graph kernels are usually defined in terms of simpler kernels over local\n",
            "substructures of the original graphs. Different kernels consider different\n",
            "types of substructures. However, in some cases they have similar predictive\n",
            "performances, probably because the substructures can be interpreted as\n",
            "approximations of the subgraphs they induce. In this paper, we propose to\n",
            "associate to each feature a piece of information about the context in which the\n",
            "feature appears in the graph. A substructure appearing in two different graphs\n",
            "will match only if it appears with the same context in both graphs. We propose\n",
            "a kernel based on this idea that considers trees as substructures, and where\n",
            "the contexts are features too. The kernel is inspired from the framework in\n",
            "[6], even if it is not part of it. We give an efficient algorithm for computing\n",
            "the kernel and show promising results on real-world graph classification\n",
            "datasets.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}